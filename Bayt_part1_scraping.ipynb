{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayt - Web Scraping, APIs, and Excel Sheet Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project by Lama Kaysi and Varun Hebbar\n",
    "\n",
    "* PLEASE NOTE: THIS NOTEBOOK TAKES A LONG TIME TO RUN DUE TO THE LARGE NUMBER OF LISTING RESULTS AND THE LARGE NUMBER OF WEBSITES WE SCRAPE/USE THE APIs FOR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook includes the following:\n",
    "\n",
    "1 - Getting user search input\n",
    "\n",
    "2 - Scraping apartments.com listings (raw JSON format & parsed)\n",
    "\n",
    "3 - Getting info from Google Maps API for each resulting listing\n",
    "\n",
    "4 - Getting info from walkscore.com API for each resulting listing (raw JSON format & parsed)\n",
    "\n",
    "5 - Getting info from howloud.com API for each resulting listing (raw JSON format & parsed\n",
    "\n",
    "6 - Scraping areavibes.com for each resulting listing (raw HTML format & parsed)\n",
    "\n",
    "7 - Creation of Excel workbook with a sheet for each of the raw and parsed info for items 2-6 described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Getting User Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method getInput() gets the search inputs from the user.\n",
    "\n",
    "These inputs include: city, state, maximum rent, minimum number of bedrooms, and minimum number of bathrooms.\n",
    "\n",
    "It returns the search inputs in the form of a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput():\n",
    "    city = input('What city do you want to rent in?').strip()\n",
    "    state = input('What is the two-letter state?').strip()\n",
    "    max_rent = input('What is your maximum rent?').strip()\n",
    "    min_rooms = input('What is the minimum number of bedrooms (0-4)?').strip()\n",
    "    min_baths = input('What is the minimum number of bathrooms (1-3)?').strip()\n",
    "    inputInfo = {'city': city,'state': state, 'rent': max_rent, 'bed': min_rooms, 'bath': min_baths}\n",
    "    return inputInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Scraping Apartments.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method searchWebsite() takes a search URL as a parameter. It goes through every resulting search page and scrapes it to retrieve information about every resulting listing that matches the user input.\n",
    "\n",
    "It returns two dicts:\n",
    "1. Contains the raw JSON for each listing\n",
    "2. Contains the parsed information from the JSON for each listing\n",
    "\n",
    "Resulting parsed information:\n",
    "\n",
    "1. URL (i.e. URL for the listing)\n",
    "2. image (i.e. an image URL for the listing)\n",
    "3. address\n",
    "4. addressLocality (i.e. city)\n",
    "5. addressRegion (i.e. state)\n",
    "6. postalCode\n",
    "7. addressCountry\n",
    "8. telephone\n",
    "9. type (i.e. type of listing â€“ e.g. apartment, house, etc.)\n",
    "10. name (i.e. address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWebsite(search_url):\n",
    "    page = requests.get(search_url)\n",
    "    page = page.content\n",
    "    soup = bs(page, 'html.parser')\n",
    "\n",
    "    af = soup.find_all('a')\n",
    "    pages = []\n",
    "    for a in af:\n",
    "        if a.has_attr('data-page'):\n",
    "            pages.append(a.text.strip())\n",
    "    af2 = []\n",
    "    for s in pages:\n",
    "        if s.isdigit() is True:\n",
    "            af2.append(s)\n",
    "    max2 = int(af2[len(af2) - 1]) + 1  # number of search result pages to iterate through + 1\n",
    "\n",
    "    links = [] # stores the listing url for each resulting listing\n",
    "    listing = {} # stores parsed information about listing from JSON\n",
    "    listing_raw = {} # stores raw JSON information\n",
    "    for page in range(1, int(max2)): # iterates through all resulting pages and scrapes them for info about listing\n",
    "        pgstr = str(page)\n",
    "        page = requests.get(search_url + pgstr + '/')\n",
    "        page = page.content\n",
    "        soup = bs(page, 'html.parser')\n",
    "        script = soup.find('script', attrs={'type': 'application/ld+json'}).text\n",
    "\n",
    "        js = json.loads(script.strip(\"'<>() \"))\n",
    "\n",
    "\n",
    "        js1 = js['about']\n",
    "\n",
    "        for item in js1: #parsing JSON for each listing on the search page\n",
    "            links.append(item.get('url'))\n",
    "            url = item.get('url')\n",
    "            image = item.get('image')\n",
    "            address = item.get('Address').get('streetAddress')\n",
    "            addressLocality = item.get('Address').get('addressLocality')\n",
    "            addressRegion = item.get('Address').get('addressRegion')\n",
    "            postalCode = item.get('Address').get('postalCode')\n",
    "            addressCountry = item.get('Address').get('addressCountry')\n",
    "            if len(item.get('telephone')) < 10:\n",
    "                telephone = 'N/A'\n",
    "            else:\n",
    "                telephone = item.get('telephone')\n",
    "            type = item.get('@type')\n",
    "            name = item.get('name')\n",
    "            aptinfo = {'url': url, 'image': image, 'address': address, 'addressLocality': addressLocality,\n",
    "                       'addressRegion': addressRegion, 'postalCode': postalCode, 'addressCountry': addressCountry,\n",
    "                       'telephone': telephone, 'type': type, 'name': name}\n",
    "\n",
    "            listing[url] = aptinfo\n",
    "            listing_raw[url] = str(item)\n",
    "\n",
    "    return listing, listing_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Google Maps API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getLatLong() method uses the Google Maps API in order to get the neighborhood, latitude, and longitude of a listing.\n",
    "\n",
    "This is a utility method used in order to allow us to search with other APIs to follow.\n",
    "\n",
    "It takes a dict as a parameter and updates it with the neighborhood, latitude, and longitude.\n",
    "\n",
    "Resulting parsed information:\n",
    "1. Neighborhood\n",
    "2. Latitude\n",
    "3. Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLatLong(info):\n",
    "    addr_split = info['address'].split()\n",
    "    city = user_info.get('city')\n",
    "    state = user_info.get('state')\n",
    "    \n",
    "    mapaddress = \"\"\n",
    "    for i in addr_split:\n",
    "        mapaddress = mapaddress + i + '%20'\n",
    "    mapsurl = 'https://maps.googleapis.com/maps/api/geocode/json?address=' + mapaddress + city +'%20'+ state + '&key=AIzaSyDq3WGNA71m5xoutY1CY094Mxq-mklHcEA'\n",
    "    \n",
    "    page = requests.get(mapsurl)\n",
    "    page = page.content\n",
    "    js = json.loads(page)\n",
    "    results = js.get('results')\n",
    "    lat = ''\n",
    "    long = ''\n",
    "    for i in results:\n",
    "        address_components2 = i.get('address_components')[2]\n",
    "        neighborhood = address_components2.get('long_name')\n",
    "        address_components7 = [d for d in i.get('address_components') if d['types'] == [\"postal_code\"]]\n",
    "        zipCode = address_components7[0].get('long_name')\n",
    "        info['postalCode'] = zipCode\n",
    "        geometry = i.get('geometry')\n",
    "        lat = geometry.get('location').get('lat')\n",
    "        long = geometry.get('location').get('lng')\n",
    "        info.update({'neighborhood':neighborhood, 'lat': lat, 'long': long})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Walkscore.com API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getScores() method uses the walkscore.com API in order to get walk score, transit score, bike score as well as the relevant text descriptions for each for a particular listing address.\n",
    "\n",
    "It takes a dict as a parameter and updates it with the above-described information. It returns a dict with the same information.\n",
    "\n",
    "Resulting parsed information:\n",
    "1. ws_link (i.e. a link to the walkscore page for that listing)\n",
    "2. walkscore\n",
    "3. ws_desc (i.e. a text description of the walkscore)\n",
    "4. transitscore\n",
    "5. ts_desc (i.e. a text description of the transit score)\n",
    "6. bikescore\n",
    "7. bs_desc (i.e. a text description of the bikescore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    addr_split = info['address'].split()\n",
    "    city = user_info.get('city')\n",
    "    state = user_info.get('state')\n",
    "    \n",
    "    mapaddress = \"\"\n",
    "    for i in addr_split:\n",
    "        mapaddress = mapaddress + i + '%20'\n",
    "    walkurl = 'http://api.walkscore.com/score?format=json&address=' + mapaddress + city +'%20'+ state + '&lat=' + str(lat) +'&lon='+ str(long)+'&transit=1&bike=1&wsapikey=aee8aec52f1b8f7c3207249a28e380bd'\n",
    "    \n",
    "    page = requests.get(walkurl)\n",
    "    page = page.content\n",
    "    js = json.loads(page)\n",
    "    walkscore = js.get('walkscore')\n",
    "   \n",
    "    ws_desc = js.get('description')\n",
    "    ws_link = js.get('ws_link')\n",
    "\n",
    "    try:\n",
    "        transitscore = js.get('transit').get('score')\n",
    "        ts_desc = js.get('transit').get('description')\n",
    "    except:\n",
    "        transitscore = 'N/A'\n",
    "        ts_desc = 'N/A'\n",
    "\n",
    "    try:\n",
    "        bikescore = js.get('bike').get('score')\n",
    "        bs_desc = js.get('bike').get('description')\n",
    "    except:\n",
    "        bikescore = 'N/A'\n",
    "        bs_desc = 'N/A'\n",
    "\n",
    "    info.update({'ws_link': ws_link, 'walkscore': walkscore, 'ws_desc': ws_desc, 'transitscore': transitscore, 'ts_desc': ts_desc, 'bikescore': bikescore, 'bs_desc': bs_desc})\n",
    "    return {'ws_link': ws_link, 'walkscore': walkscore, 'ws_desc': ws_desc, 'transitscore': transitscore, 'ts_desc': ts_desc, 'bikescore': bikescore, 'bs_desc': bs_desc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getScoresRaw() method returns the raw JSON format from the walkscore.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScoresRaw(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    addr_split = info['address'].split()\n",
    "    city = user_info.get('city')\n",
    "    state = user_info.get('state')\n",
    "    \n",
    "    mapaddress = \"\"\n",
    "    for i in addr_split:\n",
    "        mapaddress = mapaddress + i + '%20'\n",
    "    walkurl = 'http://api.walkscore.com/score?format=json&address=' + mapaddress + city +'%20'+ state + '&lat=' + str(lat) +'&lon='+ str(long)+'&transit=1&bike=1&wsapikey=aee8aec52f1b8f7c3207249a28e380bd'\n",
    "    \n",
    "    page = requests.get(walkurl)\n",
    "    data = page.text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Howloud.com API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getNoiseScores() method uses the howloud.com API in order to get information about noise levels for a listing's location and the relevant text descriptions.\n",
    "\n",
    "It takes a dict as a parameter and updates it with the above-described information. It returns a dict with the same information.\n",
    "\n",
    "Resulting parsed information:\n",
    "1. airports (i.e. noise score based on airports)\n",
    "2. traffic text (i.e. a text description of the traffic score)\n",
    "3. local text (i.e. a text description of the local noise score)\n",
    "4. airportstext (i.e. a text description of the airport noise score)\n",
    "5. score (i.e. the noise score)\n",
    "6. traffic (i.e. the noise score based on traffic)\n",
    "7. scoretext (i.e. a text description of the noise score)\n",
    "8. local (i.e. a local noise score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoiseScores(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    noise_url = 'http://elb1.howloud.com/score?key=jgYwBiiOZPxhiXxC&longitude='+str(long)+'&latitude='+str(lat)\n",
    "    page = requests.get(noise_url)\n",
    "    page = page.content\n",
    "    js = json.loads(page)\n",
    "    result = js.get('result')\n",
    "    for r in result:\n",
    "        airports = r.get('airports')\n",
    "        traffictext = r.get('traffictext')\n",
    "        localtext = r.get('localtext')\n",
    "        airportstext = r.get('airportstext')\n",
    "        score = r.get('score')\n",
    "        traffic = r.get('traffic')\n",
    "        scoretext = r.get('scoretext')\n",
    "        local = r.get('local')\n",
    "        info.update({'airports':airports, 'traffictext':traffictext, 'localtext':localtext, 'airportstext':airportstext, 'score':score, 'traffic':traffic, 'scoretext':scoretext, 'local':local})\n",
    "        return {'airports':airports, 'traffictext':traffictext, 'localtext':localtext, 'airportstext':airportstext, 'score':score, 'traffic':traffic, 'scoretext':scoretext, 'local':local}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This getNoiseScoresRaw() method returns the raw JSON format from the howloud.com API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoiseScoresRaw(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    noise_url = 'http://elb1.howloud.com/score?key=jgYwBiiOZPxhiXxC&longitude='+str(long)+'&latitude='+str(lat)\n",
    "    page = requests.get(noise_url)\n",
    "    data = page.text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Scraping areavibes.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getAreaVibesInfo() method scrapes areavibes.com and parses the HTML in order to get information about the area for a listing's location (e.g. livability, amenities, crime, etc.)\n",
    "\n",
    "It takes a dict as a parameter and updates it with the information. It returns a dict with the same information.\n",
    "\n",
    "Resulting parsed information:\n",
    "1. URL (i.e. URL for the search result)\n",
    "2. livability\n",
    "3. amenities\n",
    "4. cost of living\n",
    "5. crime\n",
    "6. education\n",
    "7. employment\n",
    "8. housing\n",
    "9. weather\n",
    "\n",
    "Note: all are scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAreaVibesInfo(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    city = user_info.get('city')\n",
    "    state = user_info.get('state')\n",
    "    postalCode = info.get('postalCode')\n",
    "    result = {}\n",
    "    neighborhood = info.get('neighborhood')\n",
    "    \n",
    "    try:\n",
    "        nei_split = neighborhood.split()\n",
    "        url_nei = ''\n",
    "        for i in nei_split:\n",
    "            url_nei = url_nei + i + '+'\n",
    "    except:\n",
    "        url_nei = ''\n",
    "    \n",
    "    addr_split = info['address'].split()\n",
    "    url_adr = ''\n",
    "    for i in addr_split:\n",
    "        url_adr = url_adr + i + '+'\n",
    "    vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/'+url_nei+'/livability/?addr='+url_adr+'&ll='+str(lat)+'+-'+str(long)\n",
    "    page = requests.get(vibes_url)\n",
    "    page = page.content\n",
    "    soup = bs(page, 'html.parser')\n",
    "    nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "    try:\n",
    "        nav_a = nav_cat.find_all('a')\n",
    "        for a in nav_a:\n",
    "            category = a.find('em').text\n",
    "            rating = a.find('i').text\n",
    "            info.update({category: rating})\n",
    "            result.update({category: rating})\n",
    "    except:\n",
    "        vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/'+url_nei+'/livability/?zip='+postalCode+'&ll='+str(lat)+'+-'+str(long)\n",
    "        page = requests.get(vibes_url)\n",
    "        page = page.content\n",
    "        soup = bs(page, 'html.parser')\n",
    "        nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "        try:\n",
    "            nav_a = nav_cat.find_all('a')\n",
    "            for a in nav_a:\n",
    "                category = a.find('em').text\n",
    "                rating = a.find('i').text\n",
    "                info.update({category: rating})\n",
    "                result.update({category: rating})\n",
    "        except:\n",
    "            vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/livability/?addr='+url_adr+'&ll='+str(lat)+'+-'+str(long)\n",
    "            page = requests.get(vibes_url)\n",
    "            page = page.content\n",
    "            soup = bs(page, 'html.parser')\n",
    "            nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "            try:\n",
    "                nav_a = nav_cat.find_all('a')\n",
    "                for a in nav_a:\n",
    "                    category = a.find('em').text\n",
    "                    rating = a.find('i').text\n",
    "                    info.update({category: rating})\n",
    "                    result.update({category: rating})\n",
    "            except:\n",
    "                category=''\n",
    "                rating=''\n",
    "                info.update({category: rating})\n",
    "                result.update({category: rating})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The getAreaVibesInfoRaw() method scrapes areavibes.com and returns the raw HTML format for each listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAreaVibesInfoRaw(info):\n",
    "    lat = info.get('lat')\n",
    "    long = info.get('long')\n",
    "    city = user_info.get('city')\n",
    "    state = user_info.get('state')\n",
    "    postalCode = info.get('postalCode')\n",
    "    \n",
    "    neighborhood = info.get('neighborhood')\n",
    "    \n",
    "    try:\n",
    "        nei_split = neighborhood.split()\n",
    "        url_nei = ''\n",
    "        for i in nei_split:\n",
    "            url_nei = url_nei + i + '+'\n",
    "    except:\n",
    "        url_nei = ''\n",
    "    \n",
    "    addr_split = info['address'].split()\n",
    "    url_adr = ''\n",
    "    for i in addr_split:\n",
    "        url_adr = url_adr + i + '+'\n",
    "    vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/'+url_nei+'/livability/?addr='+url_adr+'&ll='+str(lat)+'+-'+str(long)\n",
    "    page = requests.get(vibes_url)\n",
    "    #data = page.text\n",
    "    page = page.content\n",
    "    soup = bs(page, 'html.parser')\n",
    "    nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "    data = str(nav_cat)\n",
    "    try:\n",
    "        nav_a = nav_cat.find_all('a')\n",
    "        for a in nav_a:\n",
    "            category = a.find('em').text\n",
    "            rating = a.find('i').text\n",
    "        return data\n",
    "    except:\n",
    "        vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/'+url_nei+'/livability/?zip='+postalCode+'&ll='+str(lat)+'+-'+str(long)\n",
    "        page = requests.get(vibes_url)\n",
    "        data = page.text\n",
    "        page = page.content\n",
    "        soup = bs(page, 'html.parser')\n",
    "        nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "        data = str(nav_cat)\n",
    "        try:\n",
    "            nav_a = nav_cat.find_all('a')\n",
    "            for a in nav_a:\n",
    "                category = a.find('em').text\n",
    "                rating = a.find('i').text\n",
    "            return data\n",
    "        except:\n",
    "            vibes_url = 'http://www.areavibes.com/'+city+'-'+state+'/livability/?addr='+url_adr+'&ll='+str(lat)+'+-'+str(long)\n",
    "            page = requests.get(vibes_url)\n",
    "            data = page.text\n",
    "            page = page.content\n",
    "            soup = bs(page, 'html.parser')\n",
    "            nav_cat = soup.find('nav', attrs={'class': 'category-menu'})\n",
    "            data = str(nav_cat)\n",
    "            try:\n",
    "                nav_a = nav_cat.find_all('a')\n",
    "                for a in nav_a:\n",
    "                    category = a.find('em').text\n",
    "                    rating = a.find('i').text\n",
    "                    return data\n",
    "            except:\n",
    "                return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the above-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What city do you want to rent in?Pittsburgh\n",
      "What is the two-letter state?PA\n",
      "What is your maximum rent?2000\n",
      "What is the minimum number of bedrooms (0-4)?3\n",
      "What is the minimum number of bathrooms (1-3)?1\n"
     ]
    }
   ],
   "source": [
    "# Gets the user input (i.e. city, state, min bedrooms, min bathroom, max rent)\n",
    "user_info = getInput()\n",
    "# Appends the search URL for apartments.com with user's inputs\n",
    "search_url = 'https://www.apartments.com/'+user_info.get('city')+'-'+user_info.get('state')+'/'+user_info.get('bed')+'-bedrooms-'+user_info.get('bath')+'-bathrooms-under-'+user_info.get('rent')+'/'\n",
    "# Uses searchWebsite() method to scrape apartments.com\n",
    "listings, listing_raw = searchWebsite(search_url)\n",
    "listings_clean = copy.deepcopy(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkScoreData = {}\n",
    "walkScore = {}\n",
    "NoiseScoreData = {}\n",
    "NoiseScore = {}\n",
    "AreaScoreRaw = {}\n",
    "AreaScore = {}\n",
    "for key in listings:\n",
    "    getLatLong(listings[key])\n",
    "    walkScore[key] = getScores(listings[key])\n",
    "    walkScoreData[key] = getScoresRaw(listings[key])\n",
    "    NoiseScore[key] = getNoiseScores(listings[key])\n",
    "    NoiseScoreData[key] = getNoiseScoresRaw(listings[key])\n",
    "    AreaScore[key] = getAreaVibesInfo(listings[key])\n",
    "    AreaScoreRaw[key] = getAreaVibesInfoRaw(listings[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Creating the Excel Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an Excel workbook\n",
    "workbook = xlsxwriter.Workbook('Apartments' + user_info.get('city') +'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Worksheets - Apartments.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the info for the raw, JSON formatted information scraped from apartments.com\n",
    "worksheet3 = workbook.add_worksheet(\"listings_raw\")\n",
    "row=0\n",
    "col=0\n",
    "keys_list_3 = ['url','raw_data']\n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_3:\n",
    "    worksheet3.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in listing_raw:\n",
    "    data = listing_raw[key]\n",
    "    worksheet3.write(row, col, key)\n",
    "    col=col+1\n",
    "    worksheet3.write(row, col, data)\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the parsed info scraped from apartments.com\n",
    "worksheet1 = workbook.add_worksheet(\"listings_clean\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_1 = []\n",
    "for key in listings:\n",
    "    keys_list_1 = [x for x in listings_clean[key]]\n",
    "    break \n",
    "    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_1:\n",
    "    worksheet1.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in listings_clean:\n",
    "    dic = listings_clean[key]\n",
    "    for item in dic:\n",
    "        worksheet1.write(row, col, dic.get(item))\n",
    "        col=col+1\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Worksheets - Walkscore.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the info for the raw, JSON formatted information from the walkscore.com API\n",
    "worksheet5 = workbook.add_worksheet(\"walkscore_raw\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_5 = ['url','raw_data']\n",
    "    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_5:\n",
    "    worksheet5.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in walkScoreData:\n",
    "    data = walkScoreData[key]\n",
    "    worksheet5.write(row, col, key)\n",
    "    col=col+1\n",
    "    worksheet5.write(row, col, data)\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the parsed info from the walkscore.com API\n",
    "worksheet6 = workbook.add_worksheet(\"walkscore_clean\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_6 = []\n",
    "for key in walkScore:\n",
    "    keys_list_6 = [x for x in walkScore[key]]\n",
    "    break \n",
    "keys_list_6.insert(0,'url')    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_6:\n",
    "    worksheet6.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in walkScore:\n",
    "    dic = walkScore[key]\n",
    "    worksheet6.write(row, col, key)\n",
    "    col=col+1\n",
    "    for item in dic:\n",
    "        worksheet6.write(row, col, dic.get(item))\n",
    "        col=col+1\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Worksheets - Howloud.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the info for the raw, JSON formatted information from the howloud.com API\n",
    "worksheet7 = workbook.add_worksheet(\"noisescore_raw\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_7 = ['url','raw_data']\n",
    "    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_7:\n",
    "    worksheet7.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in NoiseScoreData:\n",
    "    data = NoiseScoreData[key]\n",
    "    worksheet7.write(row, col, key)\n",
    "    col=col+1\n",
    "    worksheet7.write(row, col, data)\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the parsed info from the howloud.com API\n",
    "worksheet8 = workbook.add_worksheet(\"noisescore_clean\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_8 = []\n",
    "for key in NoiseScore:\n",
    "    keys_list_8 = [x for x in NoiseScore[key]]\n",
    "    break \n",
    "keys_list_8.insert(0,'url')    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_8:\n",
    "    worksheet8.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in NoiseScore:\n",
    "    dic = NoiseScore[key]\n",
    "    worksheet8.write(row, col, key)\n",
    "    col=col+1\n",
    "    for item in dic:\n",
    "        worksheet8.write(row, col, dic.get(item))\n",
    "        col=col+1\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Worksheets - Areavibes.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the info for the raw, HTML information from the areavibes.com website\n",
    "worksheet9 = workbook.add_worksheet(\"AreaScore_raw\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_9 = ['url','raw_data']\n",
    "    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_9:\n",
    "    worksheet9.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in AreaScoreRaw:\n",
    "    data = AreaScoreRaw[key]\n",
    "    worksheet9.write(row, col, key)\n",
    "    col=col+1\n",
    "    worksheet9.write(row, col, data)\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding into an Excel worksheet the parsed info scraped from the areavibes.com website\n",
    "worksheet10 = workbook.add_worksheet(\"AreaScore_clean\")\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list_10 = []\n",
    "for key in AreaScore:\n",
    "    keys_list_10 = [x for x in AreaScore[key]]\n",
    "    break \n",
    "keys_list_10.insert(0,'url')    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list_10:\n",
    "    worksheet10.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in AreaScore:\n",
    "    dic = AreaScore[key]\n",
    "    worksheet10.write(row, col, key)\n",
    "    col=col+1\n",
    "    for item in dic:\n",
    "        worksheet10.write(row, col, dic.get(item))\n",
    "        col=col+1\n",
    "    row=row+1\n",
    "    col=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Worksheet - All Information Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the columns of all the previous worksheets into one overall Excel worksheet\n",
    "worksheet = workbook.add_worksheet(\"listings_merge\")\n",
    "\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "keys_list = []\n",
    "for key in listings:\n",
    "    keys_list = [x for x in listings[key]]\n",
    "    break \n",
    "    \n",
    "row=0\n",
    "col=0\n",
    "for k in keys_list:\n",
    "    worksheet.write(row,col,k)\n",
    "    col=col+1\n",
    "\n",
    "row=row+1\n",
    "col=0\n",
    "for key in listings:\n",
    "    dic = listings[key]\n",
    "    for item in dic:\n",
    "        worksheet.write(row, col, dic.get(item))\n",
    "        col=col+1\n",
    "    row=row+1\n",
    "    col=0\n",
    "    \n",
    "    \n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
